{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610f55e7-2b6e-4cf3-b743-9bc7ff528776",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baggiponte/xtream-ai-assignment/blob/main/notebooks/challenge_2-forecast.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683bde53-2f88-4bde-ab3f-a3fed6e95fd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Problem definition and constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ec56e-b938-470a-b8ad-ecf376f14d5f",
   "metadata": {},
   "source": [
    "> You are asked to develop a long-term model to predict the power load 1 year ahead. Disregard 2020, 2021, and 2022: use 2019 as test. Another piece of advice from your colleague Marta. The managers at Zap Inc are not AI experts, so they want to know how accurate your model is and why they should trust it. Be sure to answer their concerns in your notebook.\n",
    "\n",
    "Zap Inc does not provide any technical constraint. We have no indication on the model to use (e.g. whether we should opt for a more interpretable model) or whether inference time matters. No accuracy baseline or target are provided. We do not know of often it will be asked to provide forecasts or how often it will have to be retrained.\n",
    "\n",
    "The target is to provide a year-ahead forecast of the aggregate powerload: technically speaking, we shall provide a multi-step forecast from a univariate time series (and exogeous factors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4defe5-4dfe-4611-b314-6d13d43eb0b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modeling strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750df5b3-91a8-40a8-a70d-84933a138ee8",
   "metadata": {},
   "source": [
    "## Data overview and strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb6ca0-3501-4e3c-a88e-2141fb6b51c8",
   "metadata": {},
   "source": [
    "For a more thorough analysis of the data, see the first notebook. The data contains daily records of power load in Italy, aggregated. We do not know the energy sources, for example what percentage comes from renewables. If we did, we might even forecast those quantities separatedly, as renewables would be more closely tied to weather forecast. We opt not to add external data, even though this will entail higher error when, say, heat waves or rain impact the production quota from solar energy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bffe0a-9fae-4ae8-a6d3-19030811deac",
   "metadata": {},
   "source": [
    "## Modeling Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f20a1-51bd-4a1e-95f8-4eefe3a7c0fb",
   "metadata": {},
   "source": [
    "1. Provide baselines with (seasonal) naive methods.\n",
    "3. Fit a gradient boosted regression tree (GBRT): a commonly used, black-box-ish algorithm that can capture non-linear patterns in the data with little to no preprocessing\n",
    "4. Fit a harmonic regression: an additive model used in the literature, possibly easier to interpret\n",
    "\n",
    "Feature extraction will be different for the two models and will be discussed more thoroughly in the corresponding paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1e1e8-3de7-49eb-b75e-7898729ccef3",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287663f-9a83-4154-9681-d9fd98d374d5",
   "metadata": {},
   "source": [
    "We load the data and split it into two chunks: the training and test (or hold-out) set. The training set comprises data until december 2018, while 2019 data is held out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7618f-7b00-4f95-ad8c-5675e7b384a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "try:\n",
    "    from powerload import datasets\n",
    "except ImportError:\n",
    "    !pip install git+https://github.com/baggiponte/xtream-ai-assignment\n",
    "    from powerload import datasets\n",
    "\n",
    "# fetch the data\n",
    "powerload = datasets.fetch_powerload(parser=\"polars\")\n",
    "\n",
    "# save column names\n",
    "date_col, target_col = powerload.feature_names, powerload.target_names\n",
    "\n",
    "# generate train and test set\n",
    "train, test = powerload.data.filter(pl.col(date_col).dt.year().lt(2019)), powerload.data.filter(pl.col(date_col).dt.year().eq(2019))\n",
    "\n",
    "y_train, y_test = train[\"load\"].to_numpy(), test[\"load\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677891d1-289b-458e-a6a1-4e9d2f32a593",
   "metadata": {},
   "source": [
    "# Model metrics choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e541a-c4c2-4fa6-b568-0ca5a0d7f922",
   "metadata": {},
   "source": [
    "Zap Inc does not provide a KPI metric, so we are just going to minimise the forecast error. Furthermore, given the lack of indications, we are going to weight the errors uniformly. This means that we can rely on `scikit-learn` and other Python packages that provide an interface to find the optimal parameters with a default (and non-customisable) loss function. In other settings, though, we might want to value underestimates more than overestimates, or viceversa. Another common practice is to give a higher weight to the forecast error of the first $n$ months/weeks/days, or use a decay function so that the weight error of the forecast at $t + 365$ would have a smaller impact.\n",
    "\n",
    "The choice of an error metric is not trivial, though. There are three main \"mainstream\" candidates:\n",
    "\n",
    "- Mean absolute error (MAE). Averages out the error of each prediction, computed as ($|y_t - \\hat{y_t}|$). It's easy to interpret, as it preserves the scale and has a unit of measurement.\n",
    "- Mean absolute percentage error (MAPE). It's the MAE, but the error term is divided by $y_t$ to obtain a percentage. Makes it simpler to perform comparisons, however can become tricky to work with if the $y$ is zero.\n",
    "- (Root) mean squared error (R)MSE. Same as the MAE, only that the error is computed as $({y_t - \\hat{y_t}})^2$. This means that bigger errors weight more. Since the error is squared, the MSE can be much greater than the units of measurement. To shrink it, we can take its squared root.\n",
    "\n",
    "Since we are using the same units - after all, we are comparing different forecasts for the same time series, the MAE seems an adequate and interpretable choice. We can also compute the MAPE, since our datapoints are always greater than zero.\n",
    "\n",
    "Another technical note. The chosen models (GBRT, linear regression) minimise the mean squared error. Regardless, the coefficients that minimise the MSE are the same that minimise the MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102febda-90d9-44c4-8e74-d003b1d957c1",
   "metadata": {},
   "source": [
    "# Model evaluation strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e046ab-9af3-4b05-bdca-563eda535568",
   "metadata": {},
   "source": [
    "To determine the best model to test on the hold-out data, we shall perform cross validation on the training set. In other words, we shall train a model on seven consecutive years worth of data and test it on the eith: as an example, the first \"validation fold\" will range from 2006 to 2012 and 2013 will be used as validation set, the second fold will range from 2007 to 2013, while 2014 will be used for validation, and so on. The choice of a 7-year interval is somewhat arbitrary: we would like to have 5 to 10 folds to validate our model, and still have enough data for the model to fit.\n",
    "\n",
    "This strategy enables us to validate our model multiple times and average out the model prediction, to obtain a more robust estimate of the error and how the model might behave in production.\n",
    "\n",
    "Another note. Time series cross-validation can happen with or without a refit. We choose to refit the model in order to obtain multiple estimates of the coefficients and simulate what might happen in production. Technically, it'd be possible to devise a cross-validation splitter that, for example, could have the model be retrained on the first day of the month; it does not seem unlikely that such long-term predictions would be updated quite often as new data comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d3953-4dcd-4bfc-bc9a-dd6c5be92a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "\n",
    "from powerload import naive\n",
    "from powerload.model_selection import TimeSeriesCrossValidation\n",
    "\n",
    "TRAINING_WINDOW = 365 * 7\n",
    "FORECASTING_HORIZON = 365\n",
    "STRATEGY = \"rolling\"\n",
    "\n",
    "rolling_cv = TimeSeriesCrossValidation(train_size=TRAINING_WINDOW, forecasting_horizon=FORECASTING_HORIZON, strategy=STRATEGY)\n",
    "scores = [\"neg_mean_absolute_percentage_error\", \"neg_mean_absolute_error\"]\n",
    "\n",
    "def cross_val(estimator, X, y=None, scoring=scores, cv=rolling_cv, **kwargs):\n",
    "    \"\"\"Cross validation with some default values.\"\"\"\n",
    "    # estimators like `naive.*` do not need X, but X and y are needed for scoring.\n",
    "    if y is None:\n",
    "        y = X\n",
    "\n",
    "    return model_selection.cross_validate(\n",
    "        estimator=estimator,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "(num_folds := rolling_cv.get_n_splits(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430fea3-42d4-48fc-b5dc-9a3d86331587",
   "metadata": {},
   "source": [
    "With such validation scheme, we compute the error 7 times - each time on a different year. This should provide us with a better feel of the model performance on unseen data, i.e. its capability to generalise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa34df-6d4a-41a7-92e5-f5e775e2fbc9",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e57969-6d09-4839-8c0e-7f4c42cfec99",
   "metadata": {},
   "source": [
    "Our baseline is made up of two models that belong to the family of the so-called [\"naive\" models](https://otexts.com/fpp3/simple-methods.html). The forecasts of the first one are always the last value of the training set, while the second always yields the same window of length `seasonal_period`. In plain terms, the first model will return a straight line whose height is the same as the last point that was observed; whereas the second will repeat the last $n$ values observed.\n",
    "\n",
    "The chosen `seasonal_period` for the naive forecaster is of 7 days and the most reasonable to start with, though the data is also affected by quarterly patterns (e.g. consumption during winter and summer is significantly different).\n",
    "\n",
    "The choice befalls on these two because they are relatively straightforward to implement and benchmark. In other words, they might make it easier to spot how big of an accuracy improvement other machine-learning models will yield. Other baseline models could be a moving average of the past `n` observations, or additive models that use the trend and weekly (and possibly quarterly) seasonal patterns as components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957b35c-ce69-4d9e-8fc6-f34461b4f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = naive.NaiveForecaster()\n",
    "snv = naive.SeasonalNaiveForecaster(seasonal_period=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf624a-c526-4b0b-8d6a-a53b2268bc22",
   "metadata": {},
   "source": [
    "## Baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d7ed2-0108-4d0b-8aeb-f92f31caabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_results = cross_val(nv, X=train[\"load\"])\n",
    "snaive_results = cross_val(snv, X=train[\"load\"])\n",
    "\n",
    "\n",
    "def get_average_error(cv_results):\n",
    "    \"\"\"Returns average and standard deviation from an array of error metrics.\"\"\"\n",
    "    return {\n",
    "        key: (( x_ := np.abs(metrics)).mean().round(3), x_.std().round(3))\n",
    "        for key, metrics in cv_results.items()\n",
    "        if key.startswith(\"test_\")\n",
    "    }\n",
    "\n",
    "\n",
    "get_average_error(naive_results), get_average_error(snaive_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde1853-4964-42f9-9c10-741378dfa169",
   "metadata": {},
   "source": [
    "The interpretation of absolute errors is straightforward: the naive model (the first on the) has an error of approximately 16% (with a variation of ~2.5%). On average, this translates of a prediction off by 130kGHW (+/-131kGWH).\n",
    "\n",
    "The naive seasonal model's error is comparable: slightly above 15%, though with greater variance (5%), or 135kGHW (+/- 50kGWH). The greater variation signifies that there are folds where the model performs worse on the training data. This degradation is easy to explain: this naive model only contains a seasonal term, while we saw that the data displays a declining trend. Our model does not take this feature into account, so its forecast cannot decrease in time. Besides, the seasonal naive disregards other seasonal patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0303cbd-e469-4f00-85d6-3bbea6c7bb26",
   "metadata": {},
   "source": [
    "# Gradient Boosted Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f4647-58ea-468a-8f60-5be46879c3a2",
   "metadata": {},
   "source": [
    "We first train a gradient boosted regression tree (GBRT) method as it is widely used and requires little feature engineering by virtue of being a non-linear model. GBRT models performance is almost always superior both on tabular data (see [here](https://arxiv.org/abs/2207.08815)) and time-series data as well (see [here](https://arxiv.org/abs/2101.02118)). So far, GBRT models appear to best even deep learning models, with some notable exceptions (see [this](https://arxiv.org/abs/2305.14406) paper by Zalando). Since we do not have any constraints in terms of interpretability, we start with a GBRT tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac33225-8a9b-4774-915b-d72ff95a4002",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce6f97-a3b6-4346-ae10-41c81f9048ee",
   "metadata": {},
   "source": [
    "While GBRT does not generally require extensive feature engineering, we want to extract datetime components from the `date` column. This means adding numeric variables to denote the year, month and day of the week. We add a categorical variable for holidays (while an indicator variable to denote weekends was found not to improve the error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8599c5b-a958-470c-a03d-134426b0988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from powerload import preprocessing\n",
    "\n",
    "train_gbrt = (\n",
    "    train\n",
    "    .pipe(preprocessing.extract_datetime_features, \"date\", [\"year\", \"month\", \"weekday\"])\n",
    "    #.pipe(preprocessing.add_weekends, \"date\")\n",
    "    .pipe(preprocessing.add_holidays, \"date\", \"IT\")\n",
    "    .select(pl.all().exclude(\"date\", \"load\"))\n",
    ")\n",
    "\n",
    "train_gbrt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb207b-f8e0-4fee-bd0b-4b7686668a03",
   "metadata": {},
   "source": [
    "Handling of categorical variables (e.g. holidays) is quite simple. As per the docs of the `HistGradientBoostingRegressor` (bold mine):\n",
    "\n",
    "> For datasets with categorical features, using the **native categorical support is often better than relying on one-hot encoding (OneHotEncoder)**, because one-hot encoding requires more tree depth to achieve equivalent splits. It is also usually **better to rely on the native categorical support rather than to treat categorical features as continuous (ordinal) [i.e., using OrdinalEncoder]**, which happens for ordinal-encoded categorical data, since categories are nominal quantities where order does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680eee07-9b39-4f9f-87e9-a6ed3d261a05",
   "metadata": {},
   "source": [
    "## Modelling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ba8d1-7c9b-4a45-9be5-75740930d490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# store the index of the categorical column\n",
    "categorical_cols = train_gbrt.select(pl.col(pl.Categorical)).columns\n",
    "categorical_cols_idx = [train_gbrt.find_idx_by_name(col) for col in categorical_cols]\n",
    "\n",
    "# get a list of the categories\n",
    "categories = [train_gbrt[col].cat.get_categories().to_list() for col in categorical_cols]\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=categories, # categories are required as a (n_features, ) shaped array\n",
    ")\n",
    "\n",
    "transformer_gbrt = ColumnTransformer(\n",
    "    transformers=[\n",
    "            (\"holiday_encoder\", ordinal_encoder, categorical_cols_idx)\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "model_gbrt = HistGradientBoostingRegressor(\n",
    "                categorical_features=categorical_cols_idx,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "pipeline_gbrt = Pipeline(\n",
    "    steps = [\n",
    "        (\"preprocessing\", transformer_gbrt),\n",
    "        (\"gbrt\", model_gbrt)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb79b1-b8ba-4919-a8b2-0f264f6422ab",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf0bd3-eedd-474d-a587-49bdba1d512d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_gbrt = cross_val(pipeline_gbrt, train_gbrt.to_numpy(), y_train, cv=rolling_cv)\n",
    "\n",
    "get_average_error(results_gbrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bacea6-fd78-490a-91ed-ea8b18d9ebd1",
   "metadata": {},
   "source": [
    "The GBRT model returns an error that is more than halved. It averages out at 7.4%, though the standard deviation is higher than 5%. In absolute terms, the prediction, on average, is off by almost 55kGHW (+/- 30kGHW).\n",
    "\n",
    "This model is also quite simple, but its interpretation requires more effort. We cannot simply inspect the coefficients. However, we could display partial dependence or individual conditional expectation (ICE) plots [ref](https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py) to \"unbox\" predictions from the GBRT. Another, although more complex, alternative, is to use [Shapley values](https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Basic%20SHAP%20Interaction%20Value%20Example%20in%20XGBoost.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a20c01-d0d5-4634-93b4-bf93456f5dd5",
   "metadata": {},
   "source": [
    "# Harmonic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37681324-8a45-4fa7-bc3e-f03ae1605b25",
   "metadata": {},
   "source": [
    "We have outlined that the data features a linear trend; in other words, the power load series is a function of time. We can represent this with a `time` column that is basically a sequence that is 0 for the first observation and increments by 1 every step. In this way, time is represented as a step function, i.e. a line that looks like a staircase.\n",
    "\n",
    "This representation is intuitive, but its power is limited. In the forecasting literature, power series are often successfully estimated using the so called fourier (or harmonic) regression. For example, in [this paper](https://www.sciencedirect.com/science/article/pii/S2211467X20300778), the authors managed to achieve a ~3.5% error with this method when forecasting power demand from between 2012 2017 in Turkey. The authors explain that this outcome is due to Turkish households use electricity mostly for illumination purposes, and that \"in regions where electricity is used for heating, the modulated Fourier Series Expansion is not expected to achieve satisfactory performance\" due to climate conditions.\n",
    "\n",
    "But what is a harmonic regression? Simply put, it's a model with features that are a function of sine and cosine. Sine and cosine are smooth (and non-monotonic) functions that allow us to capture non-linear effects. The idea behind this modeling strategy is that sine and cosine waves add up to build the \"wave\" pattern of the target series. For another explanation, please see [the reference book](https://otexts.com/fpp3/useful-predictors.html#fourier-series) by Rob J Hyndman and George Athanasopoulos.\n",
    "\n",
    "![source: TeX StackExchange](https://i.stack.imgur.com/fxG8U.png)\n",
    "\n",
    "The tecnical formulation of such model is the following:\n",
    "\n",
    "$$\n",
    "\\hat{x}(t) = \\mu + \\delta t + \\sum_{i=1}^K\\left[\\alpha_i\\sin(2\\pi\\frac{i}{periodicity}t) + \\beta_i\\cos(2\\pi\\frac{i}{periodicity}t)\\right]\n",
    "$$\n",
    "\n",
    "Where $\\mu$ is the intercept, $t$ is the time step, and inside the summation we find $K$ the periodic components. Hyndman and Athanasopoulos recommend $K$ to be no greater than the periodicity. Periodicity, int this case, is the seasonal period of the series. We can have as many periodic \"groups\" of harmonic components as we wish. In our case, we could try 7 and 365, i.e. weekly and annual periodicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e82671-2b10-4e07-a577-c6119b7d26f2",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c9ecd-3c97-41f7-8f2c-df28bad41c17",
   "metadata": {},
   "source": [
    "Creating these harmonic features is quite easy, but we leave the implementation to the dedicated `powerload.preprocessing` module.\n",
    "\n",
    "One more thing. Since the model will be a linear regression, we should treat the `holiday` column differently. We cannot use a ordinal encoder, because encoding holidays as integers implies cardinality as well - in other words, it binds the values to a numerical sequence of evenly spaced points, which is an assumption we cannot make (technically speaking, we could move this critique to the GBRT model as well, since we passed the `year`, `month` and `weekday` features as integer variables - though this is arguably less problematic, given the non-parametric non-linear nature of the model).\n",
    "\n",
    "We could use a different strategy, named one-hot encoding, that turning every category in the `holiday` column into its own column. Every observation in the new column is assigned a zero except when the holiday falls on the correct date. This approach works, except when the resulting dataset is \"sparse\", i.e. contains a lot of zeros. This will be our case: since each holiday occurs once a year, and our training splits are 7 years long, we would only have 7 nonzero rows out of more than 2500.\n",
    "\n",
    "For this reason, it is best to turn the column into an indicator variable, i.e. a variable that takes value `1` on holidays and `0` otherwise. Keep in mind that this might cause problems when a holiday falls on a weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76620b8b-8b44-406d-b8c5-6d77f7c8866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_harmonic = (\n",
    "    train\n",
    "    .pipe(preprocessing.add_time_steps)\n",
    "    .pipe(preprocessing.add_holidays, \"date\", \"IT\")\n",
    "    .with_columns(pl.when(pl.col(\"holiday\").eq(\"No\")).then(0).otherwise(1).alias(\"is_holiday\"))\n",
    "    # .pipe(preprocessing.add_weekends, \"date\")\n",
    "    .pipe(preprocessing.add_fourier_terms, 3, 7)\n",
    "    .pipe(preprocessing.add_fourier_terms, 20, 365)\n",
    "    .select(pl.all().exclude(\"date\", \"load\", \"holiday\"))\n",
    ")\n",
    "\n",
    "train_harmonic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749cd98-28cd-43ef-963f-20f00e35b9f3",
   "metadata": {},
   "source": [
    "## Model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc628fc-e6e1-4d5b-bf22-08e104c07236",
   "metadata": {},
   "source": [
    "The resulting design matrix cannot be used directly to fit a linear model. Since we are going to fit a linear regression, we'd benefit from bringing all the variables to the same scale. Since the harmonic components are bound to `[-1, 1]`, we standardise the `time` as well as the target columns to the same scale. In this way, the resulting model coefficients will be comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e891fd-b747-4311-9b64-69c0584cda31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "transformer_harmonic = ColumnTransformer(transformers=[\n",
    "    (\"scaling\", StandardScaler(), (1, )),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "model_harmonic = TransformedTargetRegressor(regressor=LinearRegression(fit_intercept=True), transformer=StandardScaler())\n",
    "\n",
    "pipeline_harmonic = Pipeline(\n",
    "    steps=[\n",
    "        (\"standardise\", transformer_harmonic),\n",
    "        (\"harmonic_linreg\", model_harmonic)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c03f0-c726-4ec7-bff7-436aade51843",
   "metadata": {},
   "source": [
    "## Model scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea7912-3435-4772-b47c-9db9ec033a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_harmonic = cross_val(\n",
    "    pipeline_harmonic,\n",
    "    train_harmonic.to_numpy(),\n",
    "    y_train,\n",
    "    cv=rolling_cv,\n",
    "    return_estimator=True\n",
    ")\n",
    "\n",
    "get_average_error(results_harmonic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41ea3d-fc4f-499a-a31c-e2a4ccab8585",
   "metadata": {},
   "source": [
    "The average error is 5.4% (39kGHW) +/-3% (15kGHW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44fa59-e80b-4089-bc74-2f429c7f46a0",
   "metadata": {},
   "source": [
    "## Model inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1997fc3-2c90-4260-8a48-afd98b41f679",
   "metadata": {},
   "source": [
    "The advantage of a linear model is that it becomes more straighforward to interpret the coefficients. On the other hand, this fourier regression has almost 50 coefficients and the meaning of each sine/cosine pair is a bit unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1ae69-0c94-4f23-a8bc-3564109d27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_coefs_harmonic = np.vstack([\n",
    "        np.hstack([(reg := pipe[\"harmonic_linreg\"].regressor_).intercept_, reg.coef_]).reshape(1, -1)\n",
    "        for pipe\n",
    "        in results_harmonic[\"estimator\"]\n",
    "])\n",
    "\n",
    "coefs_harmonic = pl.DataFrame(\n",
    "    data = [\n",
    "        [\"intercept\"] + train_harmonic.columns,\n",
    "        _coefs_harmonic.mean(axis=0),\n",
    "        _coefs_harmonic.std(axis=0)\n",
    "    ],\n",
    "    schema=[\"param\", \"mean\", \"std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a4726-55ab-4f32-aa0c-4ca040f2abd1",
   "metadata": {},
   "source": [
    "We can still draw a histogram of these coefficients to see which ones have the biggest effect on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf37216-f276-48bd-b970-4a4dab0f2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 20))\n",
    "\n",
    "_ = ax.barh(\n",
    "    y=coefs_harmonic[\"param\"].to_numpy()[::-1],\n",
    "    width=coefs_harmonic[\"mean\"].to_numpy()[::-1],\n",
    "    xerr=coefs_harmonic[\"std\"].to_numpy()[::-1],\n",
    "    ecolor=\"red\",\n",
    "    error_kw=dict(elinewidth=3),\n",
    ")\n",
    "\n",
    "_ = ax.axvline(x=0, c=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06192d7-0441-4c56-94fa-79aca6048f0b",
   "metadata": {},
   "source": [
    "The graph above displays the average coefficient from the different estimates across cross-validation. Errorbars are also draw using the standard deviation (though this is not the correct way of estimating the confidence intervals for the coefficients, it can provide an intuition for which parameters should be investigated).\n",
    "\n",
    "At first glance, we can see that most of these coefficients are different from zero. Most notably, the holiday dummy variable appears to be insignificant. We expected the `time` coefficient to be negative, as there is a declining trend in the data. This, combined with the retrains, also causes the high uncertainty around the intercept.\n",
    "\n",
    "Almost all weekly harmonic component have a sizeable effect. Weekly effects have the strongest effect. Yearly components are much harder to reason about - technically, there could have been almost 180 for each of sine and cosine, rendering interpretation of each one basically impossible. Perhaps different seasonal patterns could be used, such as monthly (T=28) or quarterly (T=~120)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268441c-d6d2-4076-ae0d-97371f159f43",
   "metadata": {},
   "source": [
    "# In-sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ca6f7-5240-4d87-97cd-3c5dd44ae69a",
   "metadata": {},
   "source": [
    "The `cross_val_predict` interface in scikit-learn does not allow inference with window-based splits (see [here](https://stackoverflow.com/a/43279634/12445701)), so for the moment we shall just inspect in-sample errors after fitting the model to the whole training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e3044-2d9d-4ce5-aa04-341d901d0d14",
   "metadata": {},
   "source": [
    "## Harmonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df2036-7cbe-4e3b-aa03-5b8ee0f0a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "_ = pipeline_harmonic.fit(train_harmonic.to_numpy(), y_train)\n",
    "\n",
    "insample_pred_harmonic = pipeline_harmonic.predict(train_harmonic.to_numpy())\n",
    "\n",
    "print(f\"Harmonic insample mean absolute error: {mean_absolute_error(y_train, insample_pred_harmonic):,.0f} GHW ({mean_absolute_percentage_error(y_train, insample_pred_harmonic):.1%})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fff6451-ab72-4696-a544-7a6aa2602557",
   "metadata": {},
   "source": [
    "We report the error just for informational purposes. Minimising the in-sample error would yield to overfitting; on the other hand, it's helpful to see whether in-sample and validation error are close. Conversely, if in-sample error were significantly lower, it might hint that our model overfit.\n",
    "\n",
    "However, if we plot the predictions, we find out that the model systematically overestimates the decrease in demand on holidays. This is due to its linear and additive specification. We also see that the model underestimates surges.\n",
    "\n",
    "If we look at the residuals (i.e., the difference between the predictions and the true values), we can confidently say there is information left behind. The residuals display non-stationary behavior and are serially correlated. Errors appear to be normally distributed, though the distribution is uneven and skewed. The is still signal to extract - possibly from autoregressive components, but perhaps from modeling the error as a ARIMA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab6c2b-6276-4f83-9d47-73624798e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from powerload.diagnostics import plot_predictions\n",
    "\n",
    "plot_predictions(y_train, insample_pred_harmonic, dates=train[\"date\"], suptitle=\"Harmonic regression in-sample predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6ab3f-0218-4b1c-934e-4e345890b0f1",
   "metadata": {},
   "source": [
    "## GBRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abad819-9617-42b1-9351-1c92b5be10b6",
   "metadata": {},
   "source": [
    "On the other hand, if we inspect the in-sample error in the GBRT model, we notice that it is significantly lower than the cross-validation one - perhaps because it was trained on much more data (almost twice as many observations).\n",
    "\n",
    "The non-linear nature of the gradient-boosted trees, though, meant that the model does not under-estimate power supply as the harmonic model does. The model under-estimates the extreme peaks, but less than the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf51b1-7f94-4bde-b6fb-5fee40436122",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pipeline_gbrt.fit(train_gbrt.to_numpy(), y_train)\n",
    "insample_pred_gbrt = pipeline_gbrt.predict(train_gbrt.to_numpy()) \n",
    "\n",
    "print(f\"GBRT insample error: {mean_absolute_error(y_train, insample_pred_gbrt):,.0f} GHW ({mean_absolute_percentage_error(y_train, insample_pred_gbrt):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4c25c-ade8-4541-a95e-3818ef8796e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_train, insample_pred_gbrt, dates=train[\"date\"], suptitle=\"GBRT in-sample predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de20ff-7864-4a67-b809-1681f278fe9c",
   "metadata": {},
   "source": [
    "This suggests that we try different validation strategies, perhaps with longer but more frequent training windows (e.g. monthly retraining of windows of 10+ years). Still, the residuals display non-stationary behavior and, though normally distributed, appear skewed. Autocorrelation warrants more inspection: the model might benefit from the inclusion of lagged terms, though this poses challenges of its own in inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892879b2-3834-4e4c-97bc-3ff2a770c93d",
   "metadata": {},
   "source": [
    "# Performance on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5c364-c728-4421-afc4-bf9fb392a3f0",
   "metadata": {},
   "source": [
    "## Harmonic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57733cf0-8421-46af-95a2-fc580872c3f5",
   "metadata": {},
   "source": [
    "Finally, we perform data preprocessing on the test set and predict the final result. The performance is abysmal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d83163-4e9e-46b8-b032-4cad03db1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_harmonic = (\n",
    "    test\n",
    "    .pipe(preprocessing.add_time_steps)\n",
    "    .pipe(preprocessing.add_holidays, \"date\", \"IT\")\n",
    "    .with_columns(pl.when(pl.col(\"holiday\").eq(\"No\")).then(0).otherwise(1).alias(\"is_holiday\"))\n",
    "    .pipe(preprocessing.add_fourier_terms, 3, 7)\n",
    "    .pipe(preprocessing.add_fourier_terms, 20, 365)\n",
    "    .select(pl.all().exclude(\"date\", \"load\", \"holiday\"))\n",
    ")\n",
    "\n",
    "test_pred_harmonic = pipeline_harmonic.predict(test_harmonic.to_numpy())\n",
    "\n",
    "print(f\"Harmonic test error: {mean_absolute_error(y_test, test_pred_harmonic):,.0f} GHW ({mean_absolute_percentage_error(y_test, test_pred_harmonic):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37166239-db11-4e1f-a68d-167dcb9ff871",
   "metadata": {},
   "source": [
    "If we inspect the residuals, we notice a huge difference between the means of the forecast and the actual values. We are confident this is due to the harmonic model being trained on a training dataset that is too wide: the intercept coefficient is too big, while the negative coefficient of the `time` column makes the data decrease too slowly. The predictions seem lagged with respect to the actual data and shocks are not learnt properly.\n",
    "\n",
    "Besides, the residuals clearly display a seasonal pattern. The ACF plot reveals there is still influence from the seventh lag, which might suggest we did not properly model the weekly seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2d1a0-0d92-4125-ae75-ffdbacfe6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, test_pred_harmonic, dates=test[\"date\"], suptitle=\"Harmonic regression prediction on the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c05287-544c-4427-a49d-ddfa9c61ad47",
   "metadata": {},
   "source": [
    "## GBRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4c7f0-faf6-42fb-b990-bd4ec3bd587e",
   "metadata": {},
   "source": [
    "As a comparison, we also check out the performance of the gradient boosted tree. The error is significantly smaller and similar to the in-sample/validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28565d98-d39b-46ae-b6a6-ecd4f20e70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gbrt = (\n",
    "    test\n",
    "    .pipe(preprocessing.extract_datetime_features, \"date\", [\"year\", \"month\", \"weekday\"])\n",
    "    .pipe(preprocessing.add_holidays, \"date\", \"IT\")\n",
    "    .select(pl.all().exclude(\"date\", \"load\"))\n",
    ")\n",
    "\n",
    "test_pred_gbrt = pipeline_gbrt.predict(test_gbrt.to_numpy())\n",
    "\n",
    "print(f\"Harmonic test error: {mean_absolute_error(y_test, test_pred_gbrt):,.0f} GHW ({mean_absolute_percentage_error(y_test, test_pred_gbrt):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96128e06-89ea-4ba8-a727-ea5bbcee2cc2",
   "metadata": {},
   "source": [
    "We see that the model fails to take into account the peaks in power supply around July. Part of this error could possibly be improved with weather data. We see that weekly seasonal patterns are learned correctly and there are no problems with drift (which was the biggest error source for the harmonic regression). The model fails to learn the patterns around Christmas, but the residuals surely do not display seasonal patterns. Perhaps the autocorrelation could be resolved by including autoregressive terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a6491-2589-4947-a471-6c64100f7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, test_pred_gbrt, dates=test[\"date\"], suptitle=\"GBRT predictions on the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a61b7-cb7a-4354-8ac1-da8e80628069",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Further directions of analysis\n",
    "\n",
    "* Our validation strategy should be improved. It seems unrealistic to assume that Zap Inc would weight errors uniformly and would definitely retrain the model as data comes in, so we might want to validate a model on (say) monthly retrains and use a weighted error metric. In this way, we might see whether the model fails to achieve an accurate prediction in the short term (~1 month). Besides, the current cross-validation has forecasts start with the beginning of every new year, so the first month we forecast is always January. We might find new insights if we start training from different points in time. A validation strategy closer to production might help us better inform our model.\n",
    "\n",
    "* We might want to quantify the uncertainty of our predictions. GBRT can also be fit minimising quantile losses; we can minimise the 10th and 90th quantile retrieve confidence intervals for our model (e.g. [here](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py)). This can be simpler than [using MAPIE](https://mapie.readthedocs.io/en/latest/examples_regression/4-tutorials/plot_ts-tutorial.html).\n",
    "\n",
    "* Adding lagged terms to the GBRT could improve performance. Creating lagged terms introduces missing values, which GBRT can handle natively. On the other hand, adding lagged terms to the harmonic model won't be as straightforward, though it could simply be enough to drop missing values since we would be losing a few rows over seven years worth of observations. If the model were to be retrained every week or month, this should not pose a problem. On the other hand, lagged terms force us to rethink how we obtain predictions (see [here](https://skforecast.org/0.9.1/introduction-forecasting/introduction-forecasting#multi-step-forecasting) for an introduction on single-step vs multi-step forecasting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
